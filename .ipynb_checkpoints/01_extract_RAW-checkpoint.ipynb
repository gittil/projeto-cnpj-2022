{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f38147",
   "metadata": {},
   "source": [
    "importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e3263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as Func \n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d5c97",
   "metadata": {},
   "source": [
    "iniciar a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3985115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").\\\n",
    "    appName(\"CNPJ_RAW\").\\\n",
    "        config(\"spark.executer.memory\",\"1gb\").\\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e4610",
   "metadata": {},
   "source": [
    "### No site da RFB temos 10 arquivos CSV que totalizam mais de 10 Gb e mais de 51 milhões de linhas, nesse passo realizamos a carga de todos os arquivos que se encontram na pasta RAW num único DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e00fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesta pasta temos todos os arquivos brutos, da forma que vieram do site da RFB.\n",
    "path = \"/media/douglas/DATA/PRJ_CNPJ/new_data/ETL/01.RAW/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c41dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============>                                          (23 + 1) / 88]\r"
     ]
    }
   ],
   "source": [
    "csv_to_DF = spark.read.format(\"csv\")\\\n",
    "    .option(\"delimiter\",\";\")\\\n",
    "        .option(\"enconding\",\"ISO-8859-1\")\\\n",
    "            .option(\"header\",\"False\")\\\n",
    "                .option(\"inferSchema\",\"True\")\\\n",
    "                    .load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contando a quantidade de registros que temos no DF\n",
    "csv_to_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixamos que o Spark inferisse o Schema pois não vamos precisar de toda as colunas nos próximos passos.\n",
    "# Após a seleção das colunas que iremos realmente usar, faremos essa etapa do tratamento, formatando as colunas de datas, por exemplo.\n",
    "csv_to_DF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_DF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941015e2",
   "metadata": {},
   "source": [
    "Agora vamos renomear as colunas, pois o CSV original não tinha qualquer tipo de informação, limpar algumas colunas que não vamos precisar (telefone e email, por exemplo) e concatenar outras, dessa forma deixaremos nosso DF mais clean para gerar o arquivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ef1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_DF = csv_to_DF\\\n",
    "    .withColumnRenamed('_c0','CNPJ_basico')\\\n",
    "    .withColumnRenamed('_c1','CNPJ_ordem')\\\n",
    "    .withColumnRenamed('_c2','CNPJ_dv')\\\n",
    "    .withColumnRenamed('_c3','id_matriz_filial')\\\n",
    "    .withColumnRenamed('_c4','nome_fantasia')\\\n",
    "    .withColumnRenamed('_c5','sit_cad')\\\n",
    "    .withColumnRenamed('_c6','dt_cad')\\\n",
    "    .withColumnRenamed('_c7','mt_sit_cad')\\\n",
    "    .withColumnRenamed('_c8','cid_ext')\\\n",
    "    .withColumnRenamed('_c9','pais')\\\n",
    "    .withColumnRenamed('_c10','dt_inicio_at')\\\n",
    "    .withColumnRenamed('_c11','cnae_principal')\\\n",
    "    .withColumnRenamed('_c12','cnae_secundario')\\\n",
    "    .withColumnRenamed('_c13','tp_log')\\\n",
    "    .withColumnRenamed('_c14','log')\\\n",
    "    .withColumnRenamed('_c15','nm')\\\n",
    "    .withColumnRenamed('_c16','comp')\\\n",
    "    .withColumnRenamed('_c17','bairro')\\\n",
    "    .withColumnRenamed('_c18','CEP')\\\n",
    "    .withColumnRenamed('_c19','UF')\\\n",
    "    .withColumnRenamed('_c20','municipio')\\\n",
    "    .withColumnRenamed('_c21','ddd1')\\\n",
    "    .withColumnRenamed('_c22','tel1')\\\n",
    "    .withColumnRenamed('_c23','ddd2')\\\n",
    "    .withColumnRenamed('_c24','tel2')\\\n",
    "    .withColumnRenamed('_c25','ddd_fax')\\\n",
    "    .withColumnRenamed('_c26','fax')\\\n",
    "    .withColumnRenamed('_c27','email')\\\n",
    "    .withColumnRenamed('_c28','sit_esp')\\\n",
    "    .withColumnRenamed('_c29','dt_sit_esp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9154d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select no DF principal para deixar uma saída mais limpa, tirando dados sensíveis como telefone e email\n",
    "df_select = csv_to_DF.select('CNPJ_basico',\n",
    "                             'id_matriz_filial', \n",
    "                             'nome_fantasia',\n",
    "                             'sit_cad', \n",
    "                             'dt_cad', \n",
    "                             'mt_sit_cad', \n",
    "                             'dt_inicio_at', \n",
    "                             'cnae_principal', \n",
    "                             'cnae_secundario', \n",
    "                             'tp_log', \n",
    "                             'log', \n",
    "                             'nm', \n",
    "                             'comp', \n",
    "                             'bairro', \n",
    "                             'CEP', \n",
    "                             'UF', \n",
    "                             'municipio') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64fe379",
   "metadata": {},
   "source": [
    "Para facilitar as próximas manipulações dos dados iremos persistir esses dados previamente limpos em um arquivo parquet e carregar em uma pasta com o nome Bronze, após esse passo, continuaremos efetuando alguns tratamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a407e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select.write.mode(\"overwrite\").save(\"/media/douglas/DATA/PRJ_CNPJ/new_data/ETL/02.Bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
